---
title: Fitzhugh-Nagumo Bayesian Parameter Estimation Benchmarks
author: Vaibhav Dixit, Chris Rackauckas
---

```julia
using Suppressor
```

```julia
@suppress_err using DiffEqBayes
```

```julia
using DifferentialEquations
using Plots
using ContinuousTransformations
```

```julia
gr(fmt=:png)
```

### Defining the problem.

The [FitzHugh-Nagumo model](https://en.wikipedia.org/wiki/FitzHugh%E2%80%93Nagumo_model) is a simplified version of [Hodgkin-Huxley model](https://en.wikipedia.org/wiki/Hodgkin%E2%80%93Huxley_model) and is used to describe an excitable system (e.g. neuron).

```julia
fitz = @ode_def_nohes FitzhughNagumo begin
  dv = v - v^3/3 -w + l
  dw = τinv*(v +  a - b*w)
end a b τinv l
```

```julia
prob_ode_fitzhughnagumo = ODEProblem(fitz,[1.0,1.0],(0.0,10.0),[0.7,0.8,1/12.5,0.5])
@time sol = solve(prob_ode_fitzhughnagumo, Tsit5())
```

Data is genereated by adding noise to the solution obtained above.

```julia
t = collect(linspace(1,10,10))
sig = 0.20
data = convert(Array, VectorOfArray([(sol(t[i]) + sig*randn(2)) for i in 1:length(t)]))
```

### Plot of the data and the solution.

```julia
scatter(t, data[1,:])
scatter!(t, data[2,:])
plot!(sol)
```

### Priors for the parameters which will be passed for the Bayesian Inference

```julia
priors = [Truncated(Normal(1.0,0.5),0,1.5),Truncated(Normal(1.0,0.5),0,1.5),Truncated(Normal(0.0,0.5),-0.5,0.5),Truncated(Normal(0.5,0.5),0,1)]
```

## Parameter Estimation with Stan.jl backend

```julia
@time bayesian_result_stan = stan_inference(prob_ode_fitzhughnagumo,t,data,priors;num_samples=100,num_warmup=500,reltol=1e-5,abstol=1e-5,vars =(StanODEData(),InverseGamma(3,2)))
```

```julia
Mamba.describe(bayesian_result_stan.chain_results)
```

```julia
plot_chain(bayesian_result_stan)
```

## Turing.jl backend

```julia
@time bayesian_result_turing = turing_inference(prob_ode_fitzhughnagumo,Tsit5(),t,data,priors;num_samples=500)
```

```julia
Mamba.describe(bayesian_result_turing)
```

```julia
plot_chain(bayesian_result_turing)
```

## DynamicHMC.jl backend

```julia
@time bayesian_result_dynamic = dynamichmc_inference(prob_ode_fitzhughnagumo, data, priors, t, [bridge(ℝ, Segment(0.0,1.0)),bridge(ℝ,Segment(0.0,1.0)),bridge(ℝ, Segment(0.0,1.0)),bridge(ℝ,Segment(0.0,1.0))],ϵ=0.01,initial=[0.0,0.0,0.0,0.0])
```

```julia
DynamicHMC.NUTS_statistics(bayesian_result_dynamic[2]),bayesian_result_dynamic[3],mean(bayesian_result_dynamic[1][1]),mean(bayesian_result_dynamic[1][2]),mean(bayesian_result_dynamic[1][3]),mean(bayesian_result_dynamic[1][4])
```

```julia
plot1=plot(bayesian_result_dynamic[1][1],xlab="Iterations",ylab="Value")
plot2=plot(bayesian_result_dynamic[1][2],xlab="Iterations",ylab="Value")
plot3=plot(bayesian_result_dynamic[1][3],xlab="Iterations",ylab="Value")
plot4=plot(bayesian_result_dynamic[1][4],xlab="Iterations",ylab="Value")
plot(plot1,plot2,plot3,plot4)
```

# Conclusion

The FitzHugh-Nagumo problem turns out to be harder than expected, we observe quite poor results accross the three backends. Accuracy can be increased by longer iterations to ensure better convergance at the cost of time efficiency. The parameters to be estimated were `[0.7,0.8,0.08,0.5]`

Individually, Stan.jl backend takes 1.6 minutes for warmup and 12 seconds for sampling, giving `[0.99,0.90,0.07,0.46]`. Higher accuracy can be obtained with tighter priors, increase in warmup samples and adjusting the tolerance values.

Turing.jl took just over 36 seconds and gave `[0.96,1.10,0.21,0.66]` as the result. It seems to be the most inaccurate and the trace plots indicate clear non-convergance, this can be handled by increasing the sampling size for longer iterations.

Lastly, DynamicHMC.jl backend takes 3.4 minutes and gives `[0.66, 0.88, 0.09, 0.49]`. This greater accuracy is achieved by constraints on the parameters and passing reasonable initial values. The plots indicate that more iterations would be required for convergance and this can be done by explicitly passing the length of iterations to `dynamichmc_inference`.

Overall we observe some non-convergance in all the three backends and to avoid it longer iterations would be required at the cost of effiency the choice of which depends on the user.

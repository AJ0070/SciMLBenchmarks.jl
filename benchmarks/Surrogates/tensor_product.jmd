---
title: Tensor Product Function
author: Mridul Jain, Chris Rackauckas
---

## Tensor Product Function

The tensor product function is defined as:

\[ f(x) =  \prod_{i=1}^{d} \cos(a\pi x_i) \]

where:

- \(d\): Represents the dimensionality of the input vector \(x\).
- \(x_i\): Represents the \(i\)-th component of the input vector.
- \(a\): A constant parameter.

## Let's import Surrogates and Plots

```julia
using Surrogates
using Plots
using Statistics
using SurrogatesPolyChaos
using SurrogatesRandomForest
default()
```

## Define the function

```julia
function tensor_product_function(x)
    a = 0.5
    return prod(cos.(a * Ï€ * x))
end
```

## Sampling parameters for training and test data

```julia
lb = -5.0  # Lower bound of sampling range
ub = 5.0   # Upper bound of sampling range
n = 30     # Number of training points
```

## Generate training and test data

```julia
x_train = sample(n, lb, ub, SobolSample())  # Sample training data points
y_train = tensor_product_function.(x_train)  # Calculate corresponding function values
x_test = sample(1000, lb, ub, RandomSample())  # Sample larger test data set
y_test = tensor_product_function.(x_test)  # Calculate corresponding true function values
```

## Plot training and testing points

```julia
scatter(x_train, y_train, label="Training Points", xlabel="X-axis", ylabel="Y-axis", legend=:topright)
scatter!(x_test, y_test, label="Testing Points")
```

## Train the following Surrogates:

```julia
num_round = 2
alpha = 2.0
n = 6
randomforest_surrogate = RandomForestSurrogate(x_train, y_train, lb, ub, num_round = 2)
radial_surrogate = RadialBasis(x_train, y_train, lb, ub)
lobachevsky_surrogate = LobachevskySurrogate(x_train, y_train, lb, ub, alpha = 2.0, n = 6)
kriging_surrogate = Kriging(x_train, y_train, lb, ub)
poly1 = PolynomialChaosSurrogate(x_train, y_train, lb, ub)
poly2 = PolynomialChaosSurrogate(x_train, y_train, lb, ub, op = SurrogatesPolyChaos.GaussOrthoPoly(5))
```
## Obtain predictions from all surrogates for the test data

```julia
loba_pred = lobachevsky_surrogate.(x_test)  
radial_pred = radial_surrogate.(x_test)
kriging_pred = kriging_surrogate.(x_test)
random_forest_pred = randomforest_surrogate.(x_test)
poly1_pred = poly1.(x_test)
poly2_pred = poly2.(x_test)
```

## Define a function to calculate Mean Squared Error (MSE)

```julia
function calculate_mse(predictions, true_values)
    return mean((predictions .- true_values).^2)  # Calculate mean of squared errors
end
```

## Calculate MSE for all Surrogate Models

```julia
mse_loba = calculate_mse(loba_pred, y_test)
mse_krig = calculate_mse(kriging_pred, y_test)
mse_radial = calculate_mse(radial_pred, y_test)
mse_rf = calculate_mse(random_forest_pred, y_test)
mse_poly1 = calculate_mse(poly1_pred, y_test)
mse_poly2 = calculate_mse(poly2_pred, y_test)
```

## Compare the performance of all Surrogate Models

```julia
mse_values = Dict("loba" => mse_loba, "krig" => mse_krig, "radial" => mse_radial, "rf" => mse_rf, "poly1" => mse_poly1, "poly2" => mse_poly2)

# Sort the MSE values in ascending order and display them
sorted_mse = sort(collect(mse_values), by=x->x[2])
for (model, mse) in sorted_mse
    println("$model : $mse")
end
```

## Plot true function vs. model predictions

```julia
xs = lb:0.01:ub
plot(xs, tensor_product_function.(xs), label="True function", legend=:top, color=:black)
plot!(xs, lobachevsky_surrogate.(xs), label="Lobachevsky", legend=:top, color=:red)
plot!(xs, kriging_surrogate.(xs), label="Kriging", legend=:top, color=:blue)
plot!(xs, randomforest_surrogate.(xs), label="Random Forest", legend=:top, color=:green)
plot!(xs, poly1.(xs), label="Polynomial Chaos", legend=:top, color=:purple)
plot!(xs, poly2.(xs), label="Polynomial Chaos", legend=:top, color=:purple)
plot!(xs, radial_surrogate.(xs), label="Radials", legend=:top, color=:orange)
```

## Benchmarks and footer

```julia
using SciMLBenchmarks
SciMLBenchmarks.bench_footer(WEAVE_ARGS[:folder], WEAVE_ARGS[:file])
```
